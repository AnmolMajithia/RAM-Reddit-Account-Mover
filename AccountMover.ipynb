{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "synthetic-discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "isolated-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def login(op, auth):\n",
    "    \n",
    "    with open('password'+str(op)+'.txt', 'r') as f:\n",
    "        pw = f.read()\n",
    "    with open('username'+str(op)+'.txt', 'r') as f:\n",
    "        usrnm = f.read()\n",
    "        \n",
    "    headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "    \n",
    "    res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                        auth = auth,\n",
    "                        data = {'grant_type': 'password',\n",
    "                                'username': usrnm,\n",
    "                                'password': pw}, \n",
    "                        headers = headers)\n",
    "    TOKEN = res.json()['access_token']\n",
    "    headers['Authorization'] = f'bearer {TOKEN}'\n",
    "    \n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "elder-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clientid.txt', 'r') as f:\n",
    "    CLIENT_ID = f.read()\n",
    "with open('secret.txt', 'r') as f:\n",
    "    SECRET_KEY = f.read()   \n",
    "auth = requests.auth.HTTPBasicAuth(CLIENT_ID, SECRET_KEY)\n",
    "\n",
    "headers_from = login(1, auth)\n",
    "headers_to = login(2, auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-sustainability",
   "metadata": {},
   "source": [
    "## Fetch list of subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "surprising-parade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 100 subreddits\n",
      "Fetched 200 subreddits\n",
      "Fetched 300 subreddits\n",
      "Fetched 400 subreddits\n",
      "Fetched 500 subreddits\n",
      "Fetched 600 subreddits\n",
      "Fetched 700 subreddits\n",
      "Fetched 800 subreddits\n",
      "Fetched 900 subreddits\n",
      "Fetched 1000 subreddits\n",
      "Fetched 1100 subreddits\n",
      "Fetched 1200 subreddits\n",
      "Fetched 1300 subreddits\n",
      "Finished fetching 1323 subreddits\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "res3 = requests.get('https://oauth.reddit.com/subreddits/mine/subscriber',\n",
    "                   headers=headers_from, params = {'limit':'100'})\n",
    "count = 100\n",
    "\n",
    "while res3.json()['data']['dist'] != 0:\n",
    "    \n",
    "    for post in res3.json()['data']['children']:\n",
    "        df = df.append({\n",
    "            'URL': post['data']['url'],\n",
    "            'Name': post['data']['name']        \n",
    "        }, ignore_index=True)\n",
    "        \n",
    "    res3 = requests.get('https://oauth.reddit.com/subreddits/mine/subscriber',\n",
    "                        headers=headers_from, params = {'after':df.iloc[-1]['Name'],\n",
    "                                                        'limit':'100'})\n",
    "    \n",
    "    if res3.json()['data']['dist'] != 0:\n",
    "        print('Fetched',count,'subreddits')\n",
    "        count += 100\n",
    "        \n",
    "print('Finished fetching',len(df),'subreddits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-portuguese",
   "metadata": {},
   "source": [
    "# Rough Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "unlimited-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://oauth.reddit.com/api/subscribe',\n",
    "                  headers = headers_to, params = {'skip_initial_defaults':'True',\n",
    "                                               'action':'sub',\n",
    "                                               'sr':'t5_2qjpg'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cellular-union",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Not Found', 'error': 404}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "comparative-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = requests.get('https://oauth.reddit.com/r/memes/about',\n",
    "                   headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "opened-student",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t5_2qjpg'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2.json()['data']['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-abortion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
